import time
import csv
import re
from urllib.parse import urljoin

# Selenium ë° ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# --- ì„¤ì • ì •ë³´ ---
BASE_URL = "https://english.visitseoul.net"
LIST_PAGE_URL_FORMAT = BASE_URL + "/entertainment?curPage={}"
OUTPUT_FILE = "visitseoul_dt_dd_final_stable_ver2.csv"

# âš ï¸ ì•”ë¬µì /ëª…ì‹œì  ëŒ€ê¸° ì‹œê°„ ì„¤ì • (ì •ë³´ ë¡œë“œ ì‹œê°„ í™•ë³´)
IMPLICIT_WAIT_TIME = 30
EXPLICIT_WAIT_TIME = 30

# ìˆ˜ì§‘í•  ìƒì„¸ ì •ë³´ì˜ dt ì œëª© ëª©ë¡ (DTì˜ í…ìŠ¤íŠ¸ê°€ ì´ ë¬¸ìì—´ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸)
TARGET_DT_FIELDS = {
    'Hours of Operation': None,
    'Important': None,
    'Fee': None,
    # CSV í—¤ë”ì— address, transportationì´ ì†Œë¬¸ìì´ë¯€ë¡œ, 
    # dt í…ìŠ¤íŠ¸ ë§¤ì¹­ìš©ìœ¼ë¡œ ëŒ€ë¬¸ì ì‹œì‘ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    'Address': None,      
    'Transportation': None 
}

## ğŸ“š ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜ (DT/DD ê¸°ë°˜ íŒŒì‹±)
def crawl_detail_page(driver, detail_url):
    full_url = urljoin(BASE_URL, detail_url)
    print(f"  -> ìƒì„¸ í˜ì´ì§€ Selenium í¬ë¡¤ë§ ì¤‘: {full_url}")
    
    DESCRIPTION_SELECTOR = "#container > div.wide-inner > div.text-area p"
    DETAIL_CONTAINER_SELECTOR = "#container > div.detial-cont-element.active > div"
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™” (DT ë§¤ì¹­ìš©)
    result_data = {k: None for k in TARGET_DT_FIELDS} 
    
    try:
        # 1. ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ë° ëŒ€ê¸°
        driver.get(full_url)
        WebDriverWait(driver, EXPLICIT_WAIT_TIME).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, DESCRIPTION_SELECTOR))
        )
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')

    except Exception as e:
        print(f"  âŒ ìƒì„¸ í˜ì´ì§€ ë¡œë“œ/ëŒ€ê¸° ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ CSV í—¤ë”ì— ë§ëŠ” í‚¤ë¡œ None ê°’ì„ ë°˜í™˜
        return {
            'description': "ë¡œë“œ ì˜¤ë¥˜/ì‹œê°„ ì´ˆê³¼",
            'address': None, 'transportation': None,
            'image_urls': "", 'image_count': 0,
            'Hours of Operation': None, 'Important': None, 'Fee': None
        }

    # --- 1. description ì¶”ì¶œ ---
    description_tag = soup.select_one(DESCRIPTION_SELECTOR)
    description = description_tag.text.strip() if description_tag else None
    
    # --- 2. dt/dd ê¸°ë°˜ í•„ë“œ ì¶”ì¶œ ---
    detail_container = soup.select_one(DETAIL_CONTAINER_SELECTOR)
    if detail_container:
        dl_items = detail_container.find_all('dl')
        
        for dl in dl_items:
            dt_tag = dl.find('dt')
            dd_tag = dl.find('dd')
            
            if dt_tag and dd_tag:
                dt_text = dt_tag.text.strip()
                dd_text = dd_tag.text.strip()
                
                for target_field in TARGET_DT_FIELDS.keys():
                    if dt_text.startswith(target_field):
                        result_data[target_field] = dd_text
                        break

    # --- 3. Image URLs ì¶”ì¶œ ---
    image_urls_list = []
    image_items = soup.select("#container .wide-slide-element .owl-stage .owl-item > div")
    
    for item in image_items:
        style = item.get('style')
        if style:
            match = re.search(r"url\(['\"]?(.*?)['\"]?\)", style)
            if match:
                image_url = urljoin(BASE_URL, match.group(1).strip())
                image_urls_list.append(image_url)

    unique_image_urls = sorted(list(set(image_urls_list)))
    image_urls = " | ".join(unique_image_urls)
    image_count = len(unique_image_urls)
    
    time.sleep(2) 
    
    # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: CSV í—¤ë”ì— ì •ì˜ëœ ì •í™•í•œ í‚¤(address, transportation)ë¡œ ë°˜í™˜
    return {
        'description': description,
        # CSV í—¤ë”ì— ì •ì˜ëœ ì†Œë¬¸ì í‚¤ë¡œ ê°’ì„ í• ë‹¹í•©ë‹ˆë‹¤.
        'address': result_data.get('Address'), 
        'transportation': result_data.get('Transportation'),
        'image_urls': image_urls,
        'image_count': image_count,
        
        # ë‚˜ë¨¸ì§€ 3ê°œ í•„ë“œëŠ” í‚¤ê°€ CSV í—¤ë”ì™€ ì¼ì¹˜í•©ë‹ˆë‹¤.
        'Hours of Operation': result_data.get('Hours of Operation'),
        'Important': result_data.get('Important'),
        'Fee': result_data.get('Fee')
    }

## ğŸš€ ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
def main_crawler():
    page_num = 1
    # ğŸš¨ ìµœì¢… CSV í—¤ë” ëª©ë¡ ì •ì˜ (10ê°œ í•„ë“œ)
    fieldnames = [
        'title', 'url', 'description', 'address', 'transportation', 
        'image_urls', 'image_count', 'Hours of Operation', 'Important', 'Fee'
    ]
    
    try:
        options = Options()
        options.add_argument('headless') 
        options.add_argument('disable-gpu')
        options.add_argument('lang=en_US') 
        
        driver = webdriver.Chrome(options=options)
        driver.implicitly_wait(IMPLICIT_WAIT_TIME)
    except Exception as e:
        print("âŒ Selenium WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨. ChromeDriverë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
        return

    print(f"======================================")
    print(f"ğŸ” ë¹„ì§“ì„œìš¸(Visit Seoul) í¬ë¡¤ë§ ì‹œì‘...")
    print(f"======================================")

    with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        while True:
            list_page_url = LIST_PAGE_URL_FORMAT.format(page_num)
            print(f"\n--- ğŸŒ {page_num} í˜ì´ì§€ í•­ëª© ëª©ë¡ ë¡œë”© ì¤‘ ---")
            
            try:
                driver.get(list_page_url)
                WebDriverWait(driver, EXPLICIT_WAIT_TIME).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#postSearchFrm > section > div.article-list-slide > ul > li"))
                )
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')

            except Exception:
                print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆê±°ë‚˜ ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            list_items = soup.select("#postSearchFrm > section > div.article-list-slide > ul > li")
            
            if not list_items:
                print("âœ… ë” ì´ìƒ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            for item in list_items:
                title_tag = item.select_one("a > div.infor-element > div > span.title")
                title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                
                url_tag = item.select_one("a")
                detail_url = url_tag.get('href') if url_tag else None
                
                if detail_url and title != "ì œëª© ì—†ìŒ": 
                    detail_data = crawl_detail_page(driver, detail_url)
                    
                    # ğŸš¨ final_dataëŠ” fieldnamesì˜ 10ê°œ í‚¤ë§Œ í¬í•¨í•˜ë„ë¡ ë³´ì¥ë©ë‹ˆë‹¤.
                    final_data = {
                        'title': title,
                        'url': urljoin(BASE_URL, detail_url),
                        **detail_data 
                    }
                    writer.writerow(final_data)
                
            page_num += 1
            time.sleep(5) 

    driver.quit()
    print(f"\n======================================")
    print(f"ğŸ‰ í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"======================================")

if __name__ == "__main__":
    main_crawler()